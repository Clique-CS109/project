{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender Based on kNN with User similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this notebook we will use kNN method for users to build up a recommendation system for an online bookstore.\n",
    "\n",
    "![kNN method](image/knnimage.svg)\n",
    "(image source: https://commons.wikimedia.org/wiki/File%3AKnnClassification.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, recall that the recommendation accuracy $\\Lambda$ is defined as:\n",
    "$$\\Lambda=\\frac{1}{N}\\sum_{u=1}^N\\frac{|Y_u\\cap P_u|}{|Y_u|}$$\n",
    "where $Y_u$ is the set of books that a user $u$ in the test set purchased, and $P_u$ is the set of books recommended to the user $u$, and $N$ is the total number of users in the test set.\n",
    "\n",
    "\n",
    "On this notebook there are three different methods, of which we will sketch the algorithm as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Based only on the popularity of books\n",
    "\n",
    "1. Sort all books with respect to the number of transactions of each book.\n",
    "2. For each user, just recommend the most popular books and calcuate the accuracy $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Based on kNN method (called kNN-only)\n",
    "\n",
    "Suppose we are given a user $u_0$.\n",
    "\n",
    "1. We calculate the similarity score between $u_0$ and each other user, say $u_i$, as follows.\n",
    "$$sim(u_0, u_i) = \\frac{|\\text{books that both $u_0$ and $u_i$ bought}|}{\\sqrt{|\\text{books that $u_0$ bought}|\\cdot|\\text{books that $u_i$ bought}|}}$$\n",
    "In other words, it is the cosine similarity of \"support vectors\" of two users, where the support vector of $u_0$, denoted by $v_0$, is the vector of length the same as the number of all books and has value 1 if $u_0$ bought the book on the corresponding position, and 0 elsewhere. Then the similarity score is defined by\n",
    "$$sim(u_0, u_i) = \\frac{v_0\\circ v_i}{||v_0||\\cdot||v_i||}$$\n",
    "where $v_i$ is the support vector of $u_i$.\n",
    "2. We choose $k$ number of users with the largest similarity values (except $u_0$,) denoted by $S(k, u_0)$.\n",
    "3. From this data we define a score of each book, say $b_j$, as follows.\n",
    "$$score(b_j) = \\sum_{u_i \\in S(k, u_0)} \\frac{sim(u_0, u_i)\\cdot \\delta(b_j, u_i)}{\\sqrt{|\\text{books that $u_i$ bought}|}}$$\n",
    "$$\\delta(b_j, u_i) = 1 \\text{ if $u_i$ bought $b_j$ before, } 0 \\text{ otherwise}$$\n",
    "In terms of support vectors, the \"score vector of all books\" is defined by\n",
    "$$\\overrightarrow{SCORE} = \\sum_{u_i \\in S(k, u_0)} sim(u_0, u_i)\\frac{v_i}{||v_i||}$$\n",
    "The reason of the normalization factor $||v_i||$ on the denominator is that if some user bought lots of books, we want to regard each book he/she bought less seriously.\n",
    "4. Excluding books $u_0$ already bought, recommend books with highest scores to $u_0$.\n",
    "5. Calculate the recommendation accuracy $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Based on kNN method (called weighted kNN)\n",
    "\n",
    "Suppose we are given a user $u_0$.\n",
    "\n",
    "1. We define a score of each book by the following.\n",
    "$$score(b_j) = score_{pop}(b_j) + \\alpha\\cdot score_{kNN}(b_j, k)$$\n",
    "Here\n",
    "$$score_{pop}(b_j) = \\frac{|\\text{all transactions for $b_j$}|}{|\\text{all transactions}|}$$\n",
    "and $score_{kNN}(b_j, k)$ is the score defined in the second method with given $k$.\n",
    "2. Excluding books $u_0$ already bought, recommend books with highest scores to $u_0$.\n",
    "3. Calculate the recommendation accuracy $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the third method is just the weighted sum of the first two. Meanwhile, on the second method $k$ is a hyperparameter, and on the third on both $k$ and $\\alpha$ are hyperparameters. Thus to choose these we perform grid-search on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <center> Grid-Search for kNN-only </center>\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/all_konly_grid.png\"><img style=\"float: left;\" src=\"image/for_konly_grid.png\">\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/dom_konly_grid.png\"><img style=\"float: left;\" src=\"image/sel_konly_grid.png\">\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/rel_konly_grid.png\"><img style=\"float: left;\" src=\"image/hum_konly_grid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <center> Grid-Search for weighted kNN </center>\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/all_knn_grid.jpg\" width=\"480px\"><img style=\"float: left;\" src=\"image/for_knn_grid.jpg\" width=\"480px\">\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/dom_knn_grid.jpg\" width=\"480px\"><img style=\"float: left;\" src=\"image/sel_knn_grid.jpg\" width=\"480px\">\n",
    "\n",
    "<img><img style=\"float: left;\" src=\"image/rel_knn_grid.jpg\" width=\"480px\"><img style=\"float: left;\" src=\"image/hum_knn_grid.jpg\" width=\"480px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it's time to delve into actual codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import packages that would be used throughout this notebook. We mainly use `pickle` package to import/export files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are supposed to load Spark libraries to make files used for grid search and verification. But as we will import and use precalculated ones, we just comment this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries for Spark and initialize it\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# print findspark.find()\n",
    "# import pyspark\n",
    "\n",
    "# conf=pyspark.SparkConf().setAppName('pyspark').setMaster('local[4]').set(\"spark.executor.memory\", \"4g\")\n",
    "# sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data import and refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import data to be used later. All the data are assimilated from another notebook. In order to use data from each category, just change the file names below. Also, `VAL_NUM` and `TEST_NUM` denote the number of book for each user in the validation data and the test data, respectively. It needs to be changed when we deal with another data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "VAL_NUM=7\n",
    "TEST_NUM=5\n",
    "\n",
    "traindf = pd.read_csv(\"All_train.csv\", dtype={'ISBN':np.str, 'ID':np.str, 'Date':np.str, 'Pub_Date':np.str, 'Cart_Date':np.str})\n",
    "validatedf = pd.read_csv(\"All_validate.csv\", dtype={'ISBN':np.str,'ID':np.str, 'Date':np.str,'Pub_Date':np.str,'Cart_Date':np.str})\n",
    "testdf = pd.read_csv(\"All_test.csv\", dtype={'ISBN':np.str,'ID':np.str,'Date':np.str,'Pub_Date':np.str,'Cart_Date':np.str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two more dataframes, `tr_va_df` and `alldf`, to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tr_va_df is used for testing\n",
    "tr_va_df =pd.concat([traindf, validatedf])\n",
    "# alldf is used for the final result\n",
    "alldf=pd.concat([traindf, validatedf, testdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print traindf.shape, validatedf.shape, testdf.shape, tr_va_df.shape, alldf.shape\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each book has its unique ISBN, thus in order to identify each book it is better to use ISBNs than titles. For example, '오만과 편견' ('Pride and Prejudice') by Jane Austin has several versions with different ISBN, with which we want to deal separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print traindf[traindf['Title']=='오만과 편견'].ISBN.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a problem if we use ISBNs. If we look at the data we notice that there are some ISBNs with 2 titles as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_for_each_ISBN=traindf.groupby('ISBN').Title.unique()\n",
    "title_for_each_ISBN[title_for_each_ISBN.apply(lambda x: len(x)>1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is because there are some titles that start with '[염가한정판매]', which means the book is discounted for limited time. Since it is still the same book, we simply remove '[염가한정판매]' from each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delete [염가한정판매] from title\n",
    "# Input : dataframe\n",
    "# Output: same dataframe with [염가한정판매] removed from each title\n",
    "\n",
    "def del_yeomga(df):\n",
    "    resdf=df.copy()\n",
    "    title_series=resdf['Title'].apply(lambda r: r if r[:21]!='[염가한정판매] ' else r[21:])\n",
    "    resdf['Title']=title_series\n",
    "    return resdf\n",
    "\n",
    "traindf   =del_yeomga(traindf)\n",
    "validatedf=del_yeomga(validatedf)\n",
    "testdf    =del_yeomga(testdf)\n",
    "tr_va_df  =del_yeomga(tr_va_df)\n",
    "alldf     =del_yeomga(alldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we drop multiple transactions with the same user and ISBN. It means that even if a customer buy the same books several times we consider it as a single transaction. It is because what we want to know is whether a customer likes a book enough to buy it; how many he/she buys the book is not important. By the same reason we will not use `Count` column too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop duplicates - We will NOT use \"Count\" column\n",
    "traindf   =traindf.drop_duplicates(['ID','ISBN'])\n",
    "validatedf=validatedf.drop_duplicates(['ID','ISBN'])\n",
    "testdf    =testdf.drop_duplicates(['ID','ISBN'])\n",
    "tr_va_df  =tr_va_df.drop_duplicates(['ID','ISBN'])\n",
    "alldf     =alldf.drop_duplicates(['ID','ISBN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an ISBN is not given, then we simply drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldf[pd.isnull(alldf['ISBN'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop if ISBN is not given\n",
    "traindf   =traindf[~pd.isnull(traindf['ISBN'])]\n",
    "validatedf=validatedf[~pd.isnull(validatedf['ISBN'])]\n",
    "testdf    =testdf[~pd.isnull(testdf['ISBN'])]\n",
    "tr_va_df  =tr_va_df[~pd.isnull(tr_va_df['ISBN'])]\n",
    "alldf     =alldf[~pd.isnull(alldf['ISBN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldf[pd.isnull(alldf['ISBN'])].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract columns which will be used later. (Actually `Category`, `Author`, `Publisher` columns are not needed for the algorithm, but if we want to show recommended books they would be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf   =traindf[['Title','ID','Category','Author','ISBN', 'Publisher']]\n",
    "validatedf=validatedf[['Title','ID','Category','Author','ISBN', 'Publisher']]\n",
    "testdf    =testdf[['Title','ID','Category','Author','ISBN', 'Publisher']]\n",
    "tr_va_df  =tr_va_df[['Title','ID','Category','Author','ISBN', 'Publisher']]\n",
    "alldf     =alldf[['Title','ID','Category','Author','ISBN', 'Publisher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Precalculation and its corresponding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate variables used for grid-search and recommendation. One of the benefits to calculate them in advance is that it reduces time for grid-search, which may take hours or even days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `make_vec` function is to make an indicator vector where a pair of (sublist, list) is given. For example, if we are given $([a, b, d], [a, b, c, d, e])$ then the result will be $[True, True, False, True, False]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for given items which is a subset of total, return the mask vector corresponding to the positions of items\n",
    "# Input : items: a list of some books or customers\n",
    "#         total: a list of all books or customers\n",
    "# Output: True/False vector with True in the positions corresponding to the given \"items\"\n",
    "\n",
    "def make_vec(items,total):\n",
    "    res=np.zeros(len(total), dtype=bool)\n",
    "    for i in items:\n",
    "        res[np.argwhere(total==i)[0][0]]=True\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_support_for_book` makes the dictionary where each key is an ISBN and its value is the vector which corresponds to the buyers of the book with respect to the whole customers. `compute_supports_for_customer` is similar, here each key is a customer and its value is the vector which corresponds to the books he/she bought with respect to the whole books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all books, make a list of vectors of customers who bought each book\n",
    "# Input : dataframe\n",
    "# Output: dictionary of the form \"ISBN: its support vector with respect to customers who bought it\"\n",
    "\n",
    "def compute_supports_for_book(df):\n",
    "    IDs=df.ID.unique()\n",
    "    gb_ISBN = df.groupby('ISBN').ID.unique()\n",
    "    res={}\n",
    "    for e,v in zip(gb_ISBN.index.values, gb_ISBN.values):\n",
    "        res[e] = make_vec(v,IDs)\n",
    "    return res\n",
    "\n",
    "# for all customers, make a list of vectors of books that each customer bought\n",
    "# Input : dataframe\n",
    "# Output: dictionary of the form \"User ID: its support vector with respect to books that they bought\"\n",
    "\n",
    "def compute_supports_for_customer(df):\n",
    "    ISBNs = df.ISBN.unique()\n",
    "    gb_ID = df.groupby('ID').ISBN.unique()\n",
    "    res={}\n",
    "    for e,v in zip(gb_ID.index.values, gb_ID.values):\n",
    "        res[e] = make_vec(v,ISBNs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions above, we calculate each dictionary for `traindf`, `tr_va_df`, `alldf`. Usually it takes less than 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate corresponding supports\n",
    "\n",
    "%time found_supports=compute_supports_for_customer(traindf)\n",
    "print 'found supports from train dataframe'\n",
    "\n",
    "%time found_supports_w_val=compute_supports_for_customer(tr_va_df)\n",
    "print 'found supports from train and validation dataframe'\n",
    "\n",
    "%time found_supports_all=compute_supports_for_customer(alldf)\n",
    "print 'found supports from all dataframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the proportion of transactions of each book with respect to the whole transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given dataframe, return a dictionary of proportion of each book among all the books\n",
    "# Input : dataframe\n",
    "# Output: dictionary of the form \"ISBN: its proportion among all the books\"\n",
    "\n",
    "def get_allbooks_prob(df):\n",
    "    temp=df.groupby('ISBN').ID.unique().apply(len)\n",
    "    return dict(temp.apply(lambda r: r*1./temp.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we calculate each dictionary for `traindf`, `tr_va_df`, `alldf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate proportion of each book on the whole data\n",
    "\n",
    "allbooks_prob=get_allbooks_prob(traindf)\n",
    "allbooks_prob_w_val=get_allbooks_prob(tr_va_df)\n",
    "allbooks_prob_all=get_allbooks_prob(alldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the dictionary where each user is the key and its value is the set of books he/she bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the set of bought books for each customer\n",
    "\n",
    "train_bought_set=dict(traindf.groupby('ID').ISBN.unique().apply(set))\n",
    "validate_bought_set=dict(validatedf.groupby('ID').ISBN.unique().apply(set))\n",
    "test_bought_set=dict(testdf.groupby('ID').ISBN.unique().apply(set))\n",
    "tr_va_bought_set=dict(tr_va_df.groupby('ID').ISBN.unique().apply(set))\n",
    "all_bought_set=dict(alldf.groupby('ID').ISBN.unique().apply(set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is to calculate similarity for each pair of users. If a dataframe and a user are given, it returns a list of tuples (user, the similarity value of the given user and this user) sorted by similarity values. Here we use cosine similarity. Note that the data we are dealing with is sparse, i.e. the set of books each user bought is very small compared to the whole books. Thus if we use cosine similarity the computation cost is relatively smaller than other methods, e.g. Pearson correlation. More precisely, the similarity value is defined as follows.\n",
    "\n",
    "\n",
    "$$sim(u_1, u_2) = \\frac{\\text{the number of books that $u_1, u_2$ both bought}}{\\sqrt{(\\text{the number of books that $u_1$ bought})(\\text{the number of books that $u_2$ bought})}}$$\n",
    "\n",
    "If the numerator is zero, then we define it to be zero regardless of whether or not the denominator is also zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given dataframe and userid, return the dictionary of similarity values\n",
    "# here we used the cosine value as the similarity value\n",
    "# Input : dataframe and user id\n",
    "# Output: SORTED list of tuples (User ID of another user, similarity value) if the value is NOT zero\n",
    "\n",
    "def get_sim(df, cid):\n",
    "    clist=set(df[df.ID==cid].ISBN.unique())\n",
    "    res={}\n",
    "    sup_num=len(clist)\n",
    "    \n",
    "    if sup_num==0:\n",
    "        return []\n",
    "    \n",
    "    for i, v in df.groupby('ID').ISBN.unique().iteritems():\n",
    "        if i != cid:\n",
    "            int_num=len(clist.intersection(set(v)))\n",
    "            if int_num !=0:\n",
    "                res[i]=int_num*1./(np.sqrt(sup_num*len(set(v))))\n",
    "    \n",
    "    res_transform = [(j,res[j]) for j in sorted(res, key=res.get, reverse=True)]\n",
    "    \n",
    "    return res_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are supposed to get the dictionary where keys are user IDs and values are lists of tuples as described above. Also we need to get one from each dataframe. However, it takes a lot of time (~30 min for each,) so we will use the precalculated results stored in files. Note that when the files were made (see commented cells) we used Spark to reduce processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate similarity dictionary from train data\n",
    "\n",
    "with open('sim_dict.pkl', 'rb') as fp:\n",
    "    sim_dict=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sim_dict=dict(sc.parallelize(validatedf.ID.unique(),4).map(lambda r: (r,get_sim(traindf,r))).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('sim_dict.pkl', 'wb') as fp:\n",
    "#     pickle.dump(sim_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate similarity dictionary from train and validate data\n",
    "\n",
    "with open('sim_dict_w_val.pkl', 'rb') as fp:\n",
    "    sim_dict_w_val=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sim_dict_w_val=dict(sc.parallelize(testdf.ID.unique(),4).map(lambda r: (r,get_sim(tr_va_df,r))).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('sim_dict_w_val.pkl', 'wb') as fp:\n",
    "#     pickle.dump(sim_dict_w_val,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate similarity dictionary from all data\n",
    "\n",
    "with open('sim_dict_all.pkl', 'rb') as fp:\n",
    "    sim_dict_all=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sim_dict_all=dict(sc.parallelize(testdf.ID.unique(),4).map(lambda r: (r,get_sim(alldf,r))).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('sim_dict_all.pkl', 'wb') as fp:\n",
    "#     pickle.dump(sim_dict_all,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. \"The main function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function for kNN method we use. It takes (dataframe, a list of tuples (user ID, similarity value), a dictionary of support vectors with respect to users, k for kNN method) as the input. Then the result is a dictionary where keys are ISBNs and values are scores of books for each user. The score for given book $b$ and user $u$ is defined as follows.\n",
    "\n",
    "$$score(b, u) = \\sum_{\\substack{u'\\neq u \\\\u' \\text{ bought } b}} \\frac{sim(u, u')}{\\sqrt{\\text{the number of books that $u'$ bought}}}$$\n",
    "\n",
    "Its heuristic meaning is that contribution to the score by each user should be proportional to the similarity value but inversely proportional to (the square of) the number of books he/she bought. For example, if someone bought thousands of books, then each book might not be much valuable to this customer. However, if someone bought only 2 books, then each book would be much more valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the result from the dictionary of similarity values\n",
    "# here weight is defined by \"sim_value*support_vector/||support_vector||\"; please read the document\n",
    "\n",
    "# Input : df: dataframe\n",
    "#         knn_dist: SORTED list of tuples (user ID, sim_value)\n",
    "#         found_supports: dictionary of the form \"User ID: corresponding support vector\n",
    "#         k: k for k-NN, default=5\n",
    "# Output: dictionary of the form \"ISBN: its corresponding weight for recommendation\"\n",
    "\n",
    "\n",
    "def get_result(df, knn_dist, found_supports, k=5):\n",
    "    knn=knn_dist[:k]\n",
    "    res=np.zeros(len(df.ISBN.unique()))\n",
    "    for (i,j) in knn:\n",
    "        res+=found_supports[i]*j/np.sqrt(np.sum(found_supports[i]))\n",
    "    return {k:v for (k,v) in zip(df.ISBN.unique(),res)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two different method for recommendation. The first one is to use kNN method; we defined a score for each user and book, and we recommend books with highest scores. But we need to find the best k, which we will do by grid-search on the validation data.\n",
    "\n",
    "There is another method; we calculated the proportion of transactions of each book with respect to the whole transactions. It represents the popularity of each book and the most naïve method (except random recommendation...) would be to recommend the most popular books. It is very week *per se*, but what if we mix it with the kNN method? If we add the previous score to the popularity, then wouldn't it be better? For that, we need to choose the weight of the score added to the popularity. For example, if the weight is zero then we depend only on the popularity of each book. On the other hand if the weight is $\\infty$ then we only use kNN method. For this weight, we will also use grid-search on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Grid-search function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define grid-search functions for k and weight as described above. Note that if weight is fixed by a sufficiently large number, then it can also be used for grid-serach to find k only. Also we will choose 5 books of highest scores and compare them with the validation data, even though there might be more or less than 5 books for each user in the validation data. It turned out that the results are almost the same if the number of books we choose is reasonably similar to that in the validation data, say 4~7. Also it is consistent with the current system of YES24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid search for each user\n",
    "# Input : df: dataframe\n",
    "#         testset: test set which we wish to predict\n",
    "#         iter_k: range of k\n",
    "#         iter_p: range of p\n",
    "#         found_supports: dictionary of the form \"User ID: corresponding support vector\n",
    "#         allbooks_prob: dictionary of the form \"ISBN: its proportion to the whole books\"\n",
    "#         knn_dist: SORTED list of tuples (user ID, sim_value)\n",
    "#         user_bought: list of books the user already bought\n",
    "#         all_grid_data: the data we already have from the previous grid search\n",
    "# Output: dictionary of the form \"(k,p): the number of books we correctly predicted\"\n",
    "#         If (k,p) was already in all_grid_data, it has the value -1\n",
    "\n",
    "\n",
    "def do_grid_search(df, testset, iter_k, iter_p, found_supports, allbooks_prob, knn_dist, user_bought, all_grid_data, num):\n",
    "    numlist={}\n",
    "    for k in iter_k:\n",
    "        knnresult=get_result(df, knn_dist, found_supports,k=k)\n",
    "        for p in iter_p:\n",
    "            res={book:(allbooks_prob[book]+knnresult[book]*p) for book in df.ISBN.unique() if book not in user_bought}\n",
    "            numlist[(k,p)]=len(set(sorted(res, key=res.get, reverse=True)[:num]).intersection(testset))\n",
    "    return numlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to plot the result of each grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the result of grid search\n",
    "# Input : the result of the grid search, range of k, range of p, mode(choose which plot to draw)\n",
    "# Output: None (it draws the corresponding plot)\n",
    "\n",
    "def plot_grid_result(plotdata, iter_k, iter_p, mode=False):\n",
    "    X=np.outer(iter_k,np.ones(len(iter_p),dtype=np.int))\n",
    "    Y=np.outer(np.ones(len(iter_k)), iter_p) #x,y for 3D plots\n",
    "    Z=[] # these are for 3D wire plot         \n",
    "    for k in iter_k:\n",
    "        Z.append([plotdata[(k,p)] for p in iter_p])\n",
    "    \n",
    "    if mode == True:\n",
    "        ax=Axes3D(plt.figure())\n",
    "        ax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=cm.coolwarm)\n",
    "        ax.view_init(70, 20) # 3D plot\n",
    "\n",
    "    ax2=Axes3D(plt.figure())\n",
    "    ax2.plot_wireframe(X,Y,Z)\n",
    "    ax2.view_init(20, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Load precalculated data from grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually grid-search takes a lot of time; we don't want to throw away results whenever we close this notebook. Thus we will store the result and use this later when we open up this notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load grid data from previous search\n",
    "with open('all_grid_data.pkl', 'rb') as fp:\n",
    "    all_grid_data=pickle.load(fp)\n",
    "with open('all_grid_data_konly.pkl', 'rb') as fp:\n",
    "    all_grid_data_konly=pickle.load(fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many results do we already have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show which data is in all_grid_data\n",
    "\n",
    "x,y=zip(*all_grid_data)\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show which data is in all_grid_data_konly\n",
    "x=[i for ((i,j),v) in all_grid_data_konly.iteritems()]\n",
    "y=np.zeros(len(x))\n",
    "plt.plot(x,y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will proceed grid-search for the tuple (k, weight) where the weight is what is multiplied to the score of kNN, which would be added to the popularity of each book. It usually takes a lot of time; calculating for 30 points usually takes ~30 minutes. But if it is already in `all_grid_data`, we will skip grid-search and use this data to plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# grid search for weighted kNN\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "if 'all_grid_data' not in globals():\n",
    "    all_grid_data={}\n",
    "\n",
    "plotdata=defaultdict(int)\n",
    "iter_k = np.linspace(10,400,40, dtype=int) # range of k\n",
    "iter_p = [0.0,0.1,0.2,0.3,0.4,0.5] #range of p\n",
    "\n",
    "iter_k_actual = [k for k in iter_k if not all((k, p) in all_grid_data for p in iter_p)]\n",
    "\n",
    "for i,cid in enumerate(validatedf.ID.unique()):\n",
    "    search_res=do_grid_search(traindf, validate_bought_set[cid], \n",
    "                          iter_k_actual, iter_p,\n",
    "                          found_supports, allbooks_prob,\n",
    "                          sim_dict[cid], train_bought_set[cid],all_grid_data, VAL_NUM)\n",
    "    for k in iter_k_actual:\n",
    "        for p in iter_p: \n",
    "            plotdata[(k,p)] +=search_res[(k,p)]\n",
    "\n",
    "    if (i+1)%100==0:\n",
    "        print i+1,\n",
    "print \n",
    "\n",
    "\n",
    "for k in iter_k:\n",
    "    for p in iter_p:\n",
    "        if (k,p) in all_grid_data:\n",
    "            plotdata[(k,p)]=all_grid_data[(k,p)]\n",
    "        \n",
    "for i in plotdata:\n",
    "    if i not in all_grid_data:\n",
    "        all_grid_data[i]=plotdata[i]\n",
    "\n",
    "max_k, max_p = min([(k,v) for k,v in all_grid_data if all_grid_data[(k,v)]==max(all_grid_data.values())])\n",
    "print 'MAX: (k,a)=%s, num=%s'%((max_k, max_p), all_grid_data[(max_k,max_p)])\n",
    "plot_grid_result(plotdata,iter_k,iter_p, mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is for grid-search to find k for kNN only method. Here the weight is fixed by $10^9$, which is sufficiently large to be regarded as infinity. This cell is similar to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# grid search for only kNN\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "if 'all_grid_data_konly' not in globals():\n",
    "    all_grid_data_konly={}\n",
    "\n",
    "plotdata=defaultdict(int)\n",
    "iter_k = np.linspace(280,300,21, dtype=int) # range of k\n",
    "iter_p = [1e9] # always fixed\n",
    "\n",
    "iter_k_actual = [k for k in iter_k if not all((k, p) in all_grid_data_konly for p in iter_p)]\n",
    "\n",
    "for i,cid in enumerate(validatedf.ID.unique()):\n",
    "    search_res=do_grid_search(traindf, validate_bought_set[cid], \n",
    "                              iter_k_actual, iter_p,\n",
    "                              found_supports, allbooks_prob,\n",
    "                              sim_dict[cid], train_bought_set[cid],all_grid_data_konly, VAL_NUM)\n",
    "    for k in iter_k_actual:\n",
    "        for p in iter_p: \n",
    "            plotdata[(k,p)] +=search_res[(k,p)]\n",
    "                \n",
    "    if (i+1)%100==0:\n",
    "        print i+1,\n",
    "print\n",
    "\n",
    "for k in iter_k:\n",
    "    for p in iter_p:\n",
    "        if (k,p) in all_grid_data_konly:\n",
    "            plotdata[(k,p)]=all_grid_data_konly[(k,p)]\n",
    "        \n",
    "for i in plotdata:\n",
    "    if i not in all_grid_data_konly:\n",
    "        all_grid_data_konly[i]=plotdata[i]\n",
    "\n",
    "        \n",
    "max_k_only, _=max(all_grid_data_konly, key=all_grid_data_konly.get)        \n",
    "print 'MAX: k=%s, num=%s'%(max_k_only, all_grid_data_konly[(max_k_only, 1e9)])        \n",
    "plot_grid_result(plotdata,iter_k,iter_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the results obtained from grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_k, max_p = min([(k,v) for k,v in all_grid_data if all_grid_data[(k,v)]==max(all_grid_data.values())])\n",
    "max_k_only= min([k for k,v in all_grid_data_konly if all_grid_data_konly[(k,v)]==max(all_grid_data_konly.values())])\n",
    "total=len(validatedf.ID.unique())*VAL_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows all the data we obtained by the grid-search for the pair (k, weight). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# draw surface plot for all the data in all_grid_data\n",
    "\n",
    "thres=0.061 # we will plot data whose values are bigger than this threshold\n",
    "\n",
    "print 'MAX: (k,a)=%s, prob=%s%%'%((max_k, max_p), all_grid_data[(max_k,max_p)]*100./total)\n",
    "drawdata={k:v*1./total for k,v in all_grid_data.iteritems() if v*1./total>thres}\n",
    "X,Y=zip(*drawdata)\n",
    "Z=drawdata.values() # collect data from all_grid_data w.r.t. certain threshold\n",
    "\n",
    "ax=Axes3D(plt.figure())\n",
    "ax.plot_trisurf(X,Y,Z, cmap=cm.coolwarm)\n",
    "ax.view_init(20, 50) # 3D trisurface plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we draw the plot obtained from grid-search for k used in kNN-only method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# draw a scatter plot for only-kNN data in all_grid_data_konly\n",
    "\n",
    "print 'MAX: k=%s, prob=%s%%'%(max_k_only, all_grid_data_konly[(max_k_only, 1e9)]*100./total)\n",
    "drawdata=[(i,v*100./total) for ((i,j),v) in all_grid_data_konly.iteritems()]\n",
    "x,y= zip(*drawdata)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x,y,'.')\n",
    "plt.axvline(max_k_only, lw=1, ls='--', color='black')\n",
    "plt.axhline(all_grid_data_konly[(max_k_only, 1e9)]*100./total, lw=1, color='black')\n",
    "plt.axvline(max_k_only, lw=0,  label='max accuracy=%.2f%%'%(all_grid_data_konly[(max_k_only, 1e9)]*100./total))\n",
    "plt.axvline(max_k_only, lw=0,  label='when k=%s'%max_k_only)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('recommendation accuracy (%)')\n",
    "plt.title(\"All Categories\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the interesting part; which one is better? As shown below, we see that if we mix kNN method and naïve popularity method, then it turned out to be *slightly* better than kNN-only method. But would it be still true if we apply this result to the test data? We will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'MAX: (k,a)=%s, prob=%s%%'%((max_k, max_p), all_grid_data[(max_k,max_p)]*100./total)\n",
    "print 'MAX: k=%s, prob=%s%%'%(max_k_only, all_grid_data_konly[(max_k_only, 1e9)]*100./total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Test the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Which method is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the result from grid-search to figure out how accurate our model is as we compare a recommendation list from our method to the actual result. First we define the test function; basically it is similar to `do_grid_search` function, but we input single (k, weight) instead of the range of k and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "# Input : df: dataframe\n",
    "#         testset: test set which we wish to predict\n",
    "#         k: the value of k in k-NN\n",
    "#         p: the value of p\n",
    "#         found_supports: dictionary of the form \"User ID: corresponding support vector\n",
    "#         allbooks_prob: dictionary of the form \"ISBN: its proportion to the whole books\"\n",
    "#         knn_dist: SORTED list of tuples (user ID, sim_value)\n",
    "#         user_bought: list of books the user already bought\n",
    "#         num: the number of books that the model recommends\n",
    "# Output: the number of books we correctly predicted\n",
    "\n",
    "def get_test_result(df, testset, k, p, found_supports, allbooks_prob, knn_dist, user_bought, num=5):\n",
    "    knnresult=get_result(df, knn_dist, found_supports,k=k)\n",
    "    res={book:(allbooks_prob[book]+knnresult[book]*p) for book in df.ISBN.unique() if book not in user_bought}\n",
    "    return len(set(sorted(res, key=res.get, reverse=True)[:num]).intersection(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we test the model on the test data. Here we compare the results from four methods, i.e. naïve popularity, ensemble of kNN and popularity, kNN-only, and random recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# test the model!\n",
    "\n",
    "pop_res=0\n",
    "knn_res=0\n",
    "rand_res=0\n",
    "knn_only_res=0\n",
    "allbooks=set(tr_va_df.ISBN.unique())\n",
    "\n",
    "max_k, max_p=max(all_grid_data, key=all_grid_data.get)\n",
    "max_k_only,_=max(all_grid_data_konly, key=all_grid_data_konly.get)\n",
    "\n",
    "for i,cid in enumerate(testdf.ID.unique()):\n",
    "    knn_res+=get_test_result(tr_va_df, test_bought_set[cid], max_k, max_p,\n",
    "                             found_supports_w_val, allbooks_prob_w_val,\n",
    "                             sim_dict_w_val[cid], tr_va_bought_set[cid], num=TEST_NUM)\n",
    "    knn_only_res+=get_test_result(tr_va_df, test_bought_set[cid], max_k_only, 1e9,\n",
    "                             found_supports_w_val, allbooks_prob_w_val,\n",
    "                             sim_dict_w_val[cid], tr_va_bought_set[cid], num=TEST_NUM)    \n",
    "    recom_base={book:allbooks_prob_w_val[book] for book in allbooks if book not in tr_va_bought_set[cid]}\n",
    "    pop_res+=len(set(sorted(recom_base, key=recom_base.get, reverse=True)[:TEST_NUM]).intersection(test_bought_set[cid]))    \n",
    "    rand_res+=len(set(np.random.choice(list(allbooks.difference(tr_va_bought_set[cid])), TEST_NUM, replace=False)).intersection(test_bought_set[cid]))\n",
    "                \n",
    "    if (i+1)%200==0:\n",
    "        print i+1,\n",
    "        \n",
    "print\n",
    "print 'popularity  :%s%%'%(pop_res*1./(len(testdf.ID.unique())*TEST_NUM)*100)\n",
    "print 'ensemble knn:%s%%'%(knn_res*1./(len(testdf.ID.unique())*TEST_NUM)*100)\n",
    "print 'knn only    :%s%%'%(knn_only_res*1./(len(testdf.ID.unique())*TEST_NUM)*100)\n",
    "print 'random      :%s%%'%(rand_res*1./(len(testdf.ID.unique())*TEST_NUM)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the kNN-only method is the best among four. But we observed that if we mix kNN scores and popularity then it gives the better result on the validation data. Why does this happen? It means that we have overfitting problem; compared to kNN-only method, we have 1 more degree of freedon as we mix two methods, so it should give us better result during validation. However, as we apply this result to the test data, it has lower prediction power than using kNN-only method. (This phenomena can be also observed if we look at each separate category.) Thus we conclude that it is better to use the kNN-only method for recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. How about books? Find similar books if one is given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of the method we have is that it can be expanded when just the single book is given. If so, then we consider an imaginary user who only bought the given book. Then we have scores for all the other books, so by sorting out the scores we will get the list of similar books to the given one. Note that the function `get_sim_for_book` below is similar to the function `get_sim` defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a given book, return a list of users with similarity value\n",
    "# Input : dataframe, ISBN\n",
    "# Output: SORTED list of tuples (User ID, similarity value) if the value is NOT zero\n",
    "\n",
    "def get_sim_for_book(df, isbn):\n",
    "    res={}\n",
    "    for i, v in df.groupby('ID').ISBN.unique().iteritems():\n",
    "        if isbn in v:\n",
    "            res[i]=1./(np.sqrt(len(set(v))))\n",
    "    res_transform = [(j,res[j]) for j in sorted(res, key=res.get, reverse=True)]\n",
    "    return res_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a given book, we print out the list of similar books. Here we choose '아프니까 청춘이다', once a famous self-development book in Korea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for a given ISBN, return similar books with their similarity values\n",
    "\n",
    "given_ISBN='9788965700036'\n",
    "given_ISBN='9788937486067'\n",
    "num=20 # the number of books we want to recommend\n",
    "allbooks=set(alldf.ISBN.unique())\n",
    "\n",
    "max_k_only, _=max(all_grid_data_konly, key=all_grid_data_konly.get)\n",
    "\n",
    "sim=get_sim_for_book(alldf,given_ISBN)\n",
    "\n",
    "knnresult=get_result(alldf,sim, found_supports_all,k=max_k_only)\n",
    "recom_knn={book:knnresult[book] for book in allbooks if book !=given_ISBN}\n",
    "\n",
    "print 'Given book: ISBN %s, \"%s\"'%(given_ISBN, alldf[alldf['ISBN']==given_ISBN].Title.iloc[0])\n",
    "\n",
    "res_list=[]\n",
    "for (n,i) in enumerate(sorted(recom_knn, key=recom_knn.get, reverse=True)[:20]):\n",
    "    book_data=alldf[alldf['ISBN']==i].iloc[0]\n",
    "    res_list.append(dict(rank=n+1, ISBN=i, score=recom_knn[i], \n",
    "                         title=book_data.Title, author=book_data.Author, publisher=book_data.Publisher))\n",
    "pd.DataFrame(res_list).set_index('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Make a list of recommendation for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a file which contains a list of recommendation for each user. It would be used if we want to apply the kNN-only method we constructed to the real world. Here we will get lists for users only in the test data. The following function will do the job for us, and note that it is very similar to `get_test_result` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a user in the test set, return a list of recommendation\n",
    "# Input : df: dataframe\n",
    "#         k: k for kNN\n",
    "#         found_supports: dictionary of the form \"User ID: corresponding support vector\n",
    "#         knn_dist: SORTED list of tuples (user ID, sim_value)\n",
    "#         user_bought: list of books the user already bought\n",
    "#         num: the number of books that the model recommends\n",
    "# Output: a SORTED list of num-ISBNs with highest probabilities\n",
    "\n",
    "def get_final_output(df, k, found_supports, knn_dist, user_bought, num=5):\n",
    "    knnresult=get_result(df, knn_dist, found_supports,k=k)\n",
    "    res={book:knnresult[book] for book in df.ISBN.unique() if book not in user_bought}\n",
    "    return sorted(res, key=res.get, reverse=True)[:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a dictionary where keys are ISBNs and values are titles. It takes 3~4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a dictionary of the form \"ISBN: title\"\n",
    "%time titlelist={isbn:alldf[alldf['ISBN']==isbn].Title.iloc[0] for isbn in alldf.ISBN.unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we produce the final output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# making a list of recommended books for each user - will be used to do the actual test\n",
    "\n",
    "num=20 # the number of books which would be contained in a recommendation list for each user\n",
    "max_k_only, _=max(all_grid_data_konly, key=all_grid_data_konly.get)\n",
    "\n",
    "final_output={}\n",
    "\n",
    "for cid in testdf.ID.unique():\n",
    "    final_output[cid]= {pr+1:{'ISBN': isbn, 'title':titlelist[isbn]} for pr,isbn in enumerate(get_final_output(alldf, max_k_only, found_supports_all ,sim_dict_all[cid], all_bought_set[cid], num=num))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and export it to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_output.json\", \"w\") as fd:\n",
    "    json.dump(final_output, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we close this notebook, don't forget to export grid-search data to files. (They are expensive!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the grid search data\n",
    "\n",
    "with open('all_grid_data.pkl', 'wb') as fp:\n",
    "    pickle.dump(all_grid_data,fp)\n",
    "\n",
    "with open('all_grid_data_konly.pkl', 'wb') as fp:\n",
    "    pickle.dump(all_grid_data_konly,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
