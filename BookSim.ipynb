{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Book Recommender Based on kNN with Book similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the detailed description of the implementation of kNN book recommender with book similarity. In summary, the between-book similarity is calculated based on the titles, authors, and publishers of books. For a given user, the books with the largest similarities with the books that the user have purchased are recommended. The weight for each of title, author, and publisher similarities is determined through the optimization of recommender accuracy in validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyyoo/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import scipy.sparse as sparse\n",
    "import sklearn.metrics as metrics\n",
    "import sys as sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will only deal with the data in 'Domestic' `Catagory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata = pd.read_csv(\"Test_Train_Sets/Dom_test.csv\")\n",
    "traindata = pd.read_csv(\"Test_Train_Sets/Dom_train.csv\")\n",
    "valdata = pd.read_csv(\"Test_Train_Sets/Dom_validate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21726, 18), (1212, 18), (1818, 18))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape, testdata.shape, valdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21726, 1212, and 1818 transactions in train, test, and validation set, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4823 different books and 8635 different users in traindata\n",
      "813 different books and 303 different users in testdata\n",
      "1070 different books and 303 different users in valdata\n"
     ]
    }
   ],
   "source": [
    "print '%d different books and %d different users in traindata' %(np.unique(traindata['Title']).shape[0],np.unique(traindata['ID']).shape[0])\n",
    "print '%d different books and %d different users in testdata' %(np.unique(testdata['Title']).shape[0],np.unique(testdata['ID']).shape[0])\n",
    "print '%d different books and %d different users in valdata' %(np.unique(valdata['Title']).shape[0],np.unique(valdata['ID']).shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't need all features in the data to calculate the similarity between books, I extracted only relavant information such as author, publisher, and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe that contains only useful informations\n",
    "usefulcol = ['Title','ID','Category','Author','ISBN','Publisher']\n",
    "dftrain = traindata[usefulcol]\n",
    "dftest = testdata[usefulcol]\n",
    "dfval = valdata[usefulcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_train = np.unique(dftrain['Title'].values)\n",
    "author_train = np.unique(dftrain['Author'].values)\n",
    "publisher_train = np.unique(dftrain['Publisher'].values)\n",
    "title_train_dict = {i:t for i,t in enumerate(title_train)}\n",
    "author_train_dict = {i:t for i,t in enumerate(author_train)}\n",
    "publisher_train_dict = {i:t for i,t in enumerate(publisher_train)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Author` field, authors are listed as a single text, for example \"Authors: Person A, Person B, and Person C/Translators: Person D\". In order to figure out whether two books share the same author, we need to change this single text to a vector of authors, such as [Person A, Person B, Person C]. In this process, I omitted translators in author list. The function below does this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def author_to_list(author):\n",
    "    #split the list of authors in a single string into a numpy list of authors. remove brackets, '저', '역', '그림', '/', and etc\n",
    "    #remove translater\n",
    "    if author != author:\n",
    "        newauthorlist = []\n",
    "    else:\n",
    "        split_list = np.array(['/'])\n",
    "        mask = np.array([s in author for s in split_list])\n",
    "        split_list = split_list[mask]\n",
    "        \n",
    "        for s in split_list:\n",
    "            author = re.sub(s,' ..',author)\n",
    "        authorlist = author.split(' ..')\n",
    "        authorlist = np.array(filter(None,authorlist))\n",
    "        \n",
    "        translatormask = np.array([(' 역' in a) or (' 공역' in a) or \n",
    "                                   (' 편역' in a) or (' 감수' in a) or \n",
    "                                   (' 옮김' in a) or (' 등역' in a) or\n",
    "                                   (' 엮음' in a) for a in authorlist])\n",
    "        authorlist = authorlist[~translatormask]\n",
    "        \n",
    "        remove_str_list = np.array(['<','>',' 그림',' 저',' 공저',' 글', ' 등저',' 공편','편'])\n",
    "        newauthorlist = np.array([])\n",
    "        for astr in authorlist:\n",
    "            alist = np.array(astr.split(','))\n",
    "            for i,a in enumerate(alist):\n",
    "                for s in remove_str_list:\n",
    "                    a = re.sub(s,'',a)\n",
    "                alist.flat[i]=a\n",
    "            newauthorlist = np.append(newauthorlist,alist)\n",
    "                \n",
    "    return newauthorlist       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm constructing the dataframe of books. The information in this dataframe will be used to calculate the similarity between books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe of all books (no duplicates)\n",
    "dfbook = dftrain[['Author','Title','Publisher','ISBN']]\n",
    "dfbook = dfbook[~dfbook.duplicated()]\n",
    "dfbook = dfbook.reset_index()\n",
    "dfbook.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Calculate author/publisher/title similarity between books "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get entire list of authors, and use this list to make a Nbooks X Nauthors matrix, called Xauthor. Each (i,j) element in the matrix takes 1 if j-th author in the list of authors is an author of i-th book. This takes long time, so saved the constructed matrix in file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2865,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_entire_list = np.array([])\n",
    "for a in author_train:\n",
    "    author_entire_list = np.append(author_entire_list,author_to_list(a))\n",
    "author_entire_list = np.unique(author_entire_list)\n",
    "author_entire_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make binary author lists: Nbooks X Nauthors matrix\n",
    "#Use author_entire_list for index-author matching\n",
    "Xauthor = np.zeros((dfbook.index.shape[0],author_entire_list.shape[0]))\n",
    "for ind in dfbook.index:\n",
    "    alist = author_to_list(dfbook.iloc[ind]['Author'])\n",
    "    for a in alist:\n",
    "        aind = np.where(author_entire_list==a)[0][0]\n",
    "        Xauthor[ind,aind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Xauthor_Dom.txt',Xauthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, make a NbooksXNpublishers matrix, called Xpublisher, where each (i,j) element in the matrix takes 1 if j-th publisher in the list of publishers is the publisher of i-th book. The matrix is saved in a file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make binary publisher lists: Nbooks X Npublihser matrix\n",
    "#use publisher_train or publisher_train_dict for index matching\n",
    "Xpublisher = np.zeros((dfbook.index.shape[0],publisher_train.shape[0]))\n",
    "for ind in dfbook.index:\n",
    "    pub = dfbook.iloc[ind]['Publisher']\n",
    "    pind = np.where(publisher_train==pub)[0][0]\n",
    "    Xpublisher[ind,pind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Xpublisher_Dom.txt',Xpublisher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtained Xauthor matrix is used to contruct NbooksXNbooks author similarity matrix. (i,j) element of the matrix takes 1 if i-th and j-th book in `bfbook` have the same author, and 0 otherwise. Publihser similarity matrix is obtained similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simmat_author = Xauthor.dot(Xauthor.T)\n",
    "simmat_publisher = Xpublisher.dot(Xpublisher.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between two authors or two publishers is defined to take either zero (different) or one (same), because how similar the names of two authors or two publishers shouldn't affect the similarity between books. On the other hand, the simlarity between two titles should be able to take any values between 0 and 1, because book title often implies the sub-category of a book, and some people have a preference in some sub-category and are likely to buy a book in the sub-category repeatedly. <br>\n",
    "The similarity between two text title can be calculated using `SequenceMatcher` function in `difflib`. Using the function, I made the Nbooks X Nbooks similarity matrix, where (i,j) element takes a value between 0 and 1 that quantifies the similarity between the titles of i-th book and j-th book in `dfbook` dataframe.<br>\n",
    "<b>This is the longest task in this algorithm.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 46min 27s, sys: 2min 30s, total: 1h 48min 57s\n",
      "Wall time: 1h 54min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Make similarity matrix for titles: Nbooks X Nbooks matrix\n",
    "#Use dfbooks for index-title matching\n",
    "Nbooks =dfbook.index.shape[0]\n",
    "simmat_title = np.zeros((Nbooks,Nbooks))\n",
    "for i in range(Nbooks):\n",
    "    simmat_title[i,i] = 1.0\n",
    "    for j in range(i+1,Nbooks):\n",
    "        simmat_title[i,j] = similar(dfbook.iloc[i]['Title'],dfbook.iloc[j]['Title'])\n",
    "        simmat_title[j,i] = simmat_title[i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('simmat_title_Dom.txt',simmat_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have title/author/publisher similarity matrices, we may combine them to make combined similarity, and based on that we can recommend few books to a given user that are the most similar to the books that the user has bought so far. The function below does that job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_based_on_similarity(dftrain,userid,Nrec,simmat,dfbook=dfbook):\n",
    "    #recommend Nrec books similar to the books that the chosen user has bought so far\n",
    "    #input: \n",
    "    # dftrain: training data\n",
    "    # userid: id of the user\n",
    "    # Nrec: Number of books to be recommended\n",
    "    # dfbook: the dataframes of books\n",
    "    # simmat: Nbooks X Nbooks similarity matrix\n",
    "    usertrans = dftrain[dftrain['ID']==userid]  #user transaction\n",
    "    usertrans = usertrans[['Author','Title','Publisher','ISBN']]\n",
    "    usertrans = usertrans[~usertrans.duplicated()]\n",
    "    excludeind = np.array([])\n",
    "    simvec = np.zeros(simmat.shape[1])\n",
    "    for ind in usertrans.index:\n",
    "        bookind = dfbook[(dfbook['Title']==usertrans[usertrans.index == ind]['Title'].values[0]) \n",
    "                         & (dfbook['Publisher']==usertrans[usertrans.index == ind]['Publisher'].values[0])].index[0]\n",
    "        simvec = simvec+simmat[bookind,:]\n",
    "        excludeind = np.append(excludeind,[bookind])\n",
    "    simvec = simvec/usertrans.index.shape[0]\n",
    "    dfsimvec = dfbook\n",
    "    dfsimvec['Similarity'] = simvec\n",
    "\n",
    "    dfsimvec = dfsimvec.iloc[filter(lambda x: x not in excludeind,dfsimvec.index)]\n",
    "    return dfsimvec.sort('Similarity',ascending=False)[:Nrec]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the function below uses the function `recommend_based_on_similarity` above to calculate the prediction accuracy, where the prediction accuracy $\\Lambda$ is defined as:\n",
    "$$\\Lambda = \\frac{1}{N} \\sum^N_{u=1} \\frac{|Y_u \\cap P_u|}{|Y_u|}$$\n",
    "where $Y_u$ is the set of books that a user $u$ in the test/validation set purchased, and $P_u$ is the set of books recommended to the user $u$, and $N$ is the number of users in the test/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_percentage(dftrain,dftest,Nrec,weightdict,simmat_author=simmat_author,\n",
    "                       simmat_title=simmat_title,simmat_publisher=simmat_publisher,dfbook=dfbook):\n",
    "    simmat = weightdict['Author']*simmat_author+weightdict['Title']*simmat_title\n",
    "    +weightdict['Publisher']*simmat_publisher\n",
    "    testusers = np.unique(dftest['ID'].values)\n",
    "    totalbooks = 0\n",
    "    totalsuccess = 0\n",
    "    for user in testusers:\n",
    "        dfrec = recommend_based_on_similarity(dftrain,user,Nrec,simmat,dfbook=dfbook)\n",
    "        dftestuser = dftest[dftest['ID']==user]\n",
    "        totalbooks = totalbooks+Nrec\n",
    "        for i in range(dfrec.shape[0]):\n",
    "            if np.any((dftestuser['Title']==dfrec.iloc[i]['Title']) \n",
    "                & (dftestuser['Publisher']==dfrec.iloc[i]['Publisher'])):\n",
    "                totalsuccess=totalsuccess+1\n",
    "    return float(totalsuccess)/float(totalbooks)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0687568756876\n",
      "CPU times: user 21.7 s, sys: 1.78 s, total: 23.5 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weightdict = {'Author':0.4,'Title':0.5,'Publisher':0.1}\n",
    "print predict_percentage(dftrain,dfval,6,weightdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Optimize Similarity Weight - Minimize prediction error in validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I did grid search to find the opitmal title/author/publisher similarity weight that minimizes the prediction error in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 30min 32s, sys: 3min 33s, total: 1h 34min 5s\n",
      "Wall time: 1h 44min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stepsize = 0.05\n",
    "\n",
    "Aws = np.arange(0,1,stepsize)\n",
    "\n",
    "accuracy_list = []\n",
    "for A in Aws:\n",
    "    Tws = np.arange(0,1-A,stepsize)\n",
    "    for T in Tws:\n",
    "        P = 1-A-T\n",
    "        weightdict = {'Author':A,'Publisher':P,'Title':T}\n",
    "        percentage = predict_percentage(dftrain,dfval,6,weightdict)\n",
    "        accuracy = weightdict\n",
    "        accuracy['accuracy'] = percentage\n",
    "        accuracy_list = accuracy_list+[accuracy]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('accuracy_dom.json', 'w') as fp:\n",
    "    json.dump(accuracy_list, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Title</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.069857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.069307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author  Publisher  Title  accuracy\n",
       "100    0.25       0.25   0.50  0.069857\n",
       "43     0.10       0.70   0.20  0.069857\n",
       "82     0.20       0.40   0.40  0.069857\n",
       "114    0.30       0.25   0.45  0.069857\n",
       "117    0.30       0.10   0.60  0.069857\n",
       "22     0.05       0.85   0.10  0.069857\n",
       "42     0.10       0.75   0.15  0.069857\n",
       "80     0.20       0.50   0.30  0.069857\n",
       "63     0.15       0.55   0.30  0.069857\n",
       "146    0.45       0.45   0.10  0.069307\n",
       "75     0.20       0.75   0.05  0.069307\n",
       "177    0.60       0.25   0.15  0.069307\n",
       "134    0.40       0.50   0.10  0.069307\n",
       "185    0.65       0.20   0.15  0.069307\n",
       "192    0.70       0.15   0.15  0.069307\n",
       "99     0.25       0.30   0.45  0.069307\n",
       "81     0.20       0.45   0.35  0.068757\n",
       "83     0.20       0.35   0.45  0.068757\n",
       "91     0.25       0.70   0.05  0.068757\n",
       "92     0.25       0.65   0.10  0.068757\n",
       "76     0.20       0.70   0.10  0.068757\n",
       "79     0.20       0.55   0.25  0.068757\n",
       "184    0.65       0.25   0.10  0.068757\n",
       "142    0.40       0.10   0.50  0.068757\n",
       "179    0.60       0.15   0.25  0.068757\n",
       "58     0.15       0.80   0.05  0.068757\n",
       "147    0.45       0.40   0.15  0.068757\n",
       "96     0.25       0.45   0.30  0.068757\n",
       "176    0.60       0.30   0.10  0.068757\n",
       "97     0.25       0.40   0.35  0.068757\n",
       "..      ...        ...    ...       ...\n",
       "13     0.00       0.35   0.65  0.050055\n",
       "12     0.00       0.40   0.60  0.050055\n",
       "11     0.00       0.45   0.55  0.050055\n",
       "10     0.00       0.50   0.50  0.050055\n",
       "8      0.00       0.60   0.40  0.050055\n",
       "165    0.55       0.45   0.00  0.050055\n",
       "7      0.00       0.65   0.35  0.050055\n",
       "6      0.00       0.70   0.30  0.050055\n",
       "5      0.00       0.75   0.25  0.050055\n",
       "4      0.00       0.80   0.20  0.050055\n",
       "3      0.00       0.85   0.15  0.050055\n",
       "2      0.00       0.90   0.10  0.050055\n",
       "16     0.00       0.20   0.80  0.050055\n",
       "17     0.00       0.15   0.85  0.050055\n",
       "18     0.00       0.10   0.90  0.050055\n",
       "19     0.00       0.05   0.95  0.050055\n",
       "20     0.05       0.95   0.00  0.050055\n",
       "34     0.05       0.25   0.70  0.050055\n",
       "35     0.05       0.20   0.75  0.050055\n",
       "36     0.05       0.15   0.80  0.050055\n",
       "39     0.10       0.90   0.00  0.050055\n",
       "57     0.15       0.85   0.00  0.050055\n",
       "74     0.20       0.80   0.00  0.050055\n",
       "90     0.25       0.75   0.00  0.050055\n",
       "1      0.00       0.95   0.05  0.050055\n",
       "119    0.35       0.65   0.00  0.050055\n",
       "132    0.40       0.60   0.00  0.050055\n",
       "144    0.45       0.55   0.00  0.050055\n",
       "155    0.50       0.50   0.00  0.050055\n",
       "0      0.00       1.00   0.00  0.000000\n",
       "\n",
       "[210 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy_list)\n",
    "accuracy_df = accuracy_df.sort('accuracy',ascending=False)\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, multiple combinations of weights achieve the maximum accuracy. So I took the average of those combinations as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Publisher': 0.48333333333333328, 'Title': 0.33333333333333331, 'accuracy': 0.069856985698569851, 'Author': 0.18333333333333338}\n"
     ]
    }
   ],
   "source": [
    "bestweightdict = dict(accuracy_df[accuracy_df['accuracy']==accuracy_df['accuracy'].max()].mean())\n",
    "print bestweightdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of Domestic Literature, I found that when the weights are 0.48 for publisher, 0.33 for title, and 0.18 for author, the prediction accuracy is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measuring Test Prediction Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I merged training and validation set to have a new bigger training set. And update the similarity matrices I've got above to take into account the books in validation sets that are not in the original training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge train and validation set\n",
    "dftrainandval = pd.concat([dftrain,dfval])\n",
    "dftrainandval = dftrainandval.reset_index()\n",
    "dftrainandval.drop('index', axis=1, inplace=True)\n",
    "\n",
    "#dataframe of all books in train and val data(no duplicates)\n",
    "dfbookall = dftrainandval[['Author','Title','Publisher','ISBN']]\n",
    "dfbookall = dfbookall[~dfbookall.duplicated()]\n",
    "dfbookall = dfbookall.reset_index()\n",
    "dfbookall.drop('index', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3022,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_entire_list_all = np.array([])\n",
    "for a in np.unique(dfbookall['Author']):\n",
    "    author_entire_list_all = np.append(author_entire_list_all,author_to_list(a))\n",
    "author_entire_list_all = np.unique(author_entire_list_all)\n",
    "author_entire_list_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make new simmat for merged dataframe\n",
    "#Make binary author lists: Nbooks X Nauthors matrix\n",
    "#Use author_entire_list for index-author matching\n",
    "Xauthor_all = np.zeros((dfbookall.index.shape[0],author_entire_list_all.shape[0]))\n",
    "for ind in range(dfbookall.shape[0]):\n",
    "    alist = author_to_list(dfbookall.iloc[ind]['Author'])\n",
    "    for a in alist:\n",
    "        aind = np.where(author_entire_list_all==a)[0][0]\n",
    "        Xauthor_all[ind,aind] = 1\n",
    "        \n",
    "#Make binary publisher lists: Nbooks X Npublihser matrix\n",
    "#use publisher_train or publisher_train_dict for index matching\n",
    "publisher_list_all = np.unique(dfbookall['Publisher'])\n",
    "Xpublisher_all = np.zeros((dfbookall.index.shape[0],publisher_list_all.shape[0]))\n",
    "for ind in range(dfbookall.shape[0]):\n",
    "    pub = dfbookall.iloc[ind]['Publisher']\n",
    "    pind = np.where(publisher_list_all==pub)[0][0]\n",
    "    Xpublisher_all[ind,pind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 54s, sys: 4.71 s, total: 18min 59s\n",
      "Wall time: 24min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Update similarity matrix for titles:\n",
    "Nbooks_all =dfbookall.index.shape[0]\n",
    "simmat_title_all = np.zeros((Nbooks_all,Nbooks_all))\n",
    "Nbooks = dfbook.index.shape[0]\n",
    "for i in range(Nbooks,Nbooks_all):\n",
    "    for j in range(Nbooks):\n",
    "        simmat_title_all[i,j] = similar(dfbookall.iloc[i]['Title'],dfbookall.iloc[j]['Title'])\n",
    "        simmat_title_all[j,i] = simmat_title_all[i,j]\n",
    "        \n",
    "for i in range(Nbooks,Nbooks_all):\n",
    "    simmat_title_all[i,i] = 1.0\n",
    "    for j in range(i+1,Nbooks_all):\n",
    "        simmat_title_all[i,j] = similar(dfbookall.iloc[i]['Title'],dfbookall.iloc[j]['Title'])\n",
    "        simmat_title_all[j,i] = simmat_title_all[i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simmat_author_all = Xauthor_all.dot(Xauthor_all.T)\n",
    "simmat_publisher_all = Xpublisher_all.dot(Xpublisher_all.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now everything is ready to calculate the test error!!!<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0255775577558\n"
     ]
    }
   ],
   "source": [
    "#Estimate Test error\n",
    "testerror = predict_percentage(dftrainandval,dftest,4,bestweightdict,\n",
    "                               simmat_author=simmat_author_all,simmat_title=simmat_title_all,\n",
    "                               simmat_publisher=simmat_publisher_all,dfbook=dfbookall)\n",
    "print testerror"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
